{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSZ9kLnHlgj"
   },
   "source": [
    "\n",
    "\n",
    "#Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtphQYeewbpV"
   },
   "outputs": [],
   "source": [
    "globals().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIVvP7yZ5dGR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "!pip install catboost\n",
    "!pip install fredapi\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "!pip install category_encoders\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# --- target encoding ---\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor, RandomForestRegressor,\n",
    "    GradientBoostingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy import sparse\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18005,
     "status": "ok",
     "timestamp": 1761929091252,
     "user": {
      "displayName": "cheikh fall",
      "userId": "01588746123551831690"
     },
     "user_tz": -60
    },
    "id": "oBKpGvNK9KZX",
    "outputId": "b2278ae3-c136-4f1f-9561-9fa736c727c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLehuKliHt2N"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyIvpLdKvMau"
   },
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "\n",
    "API_KEY = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
    "fred = Fred(api_key=API_KEY)\n",
    "def _suiv_month_id(series):\n",
    "\n",
    "    s = series.astype(str).str[:6]                     # 'YYYYMM'\n",
    "    dt = pd.to_datetime(s + \"01\", format=\"%Y%m%d\")\n",
    "    suiv= (dt.dt.to_period(\"M\") + 1).dt.strftime(\"%Y%m\")\n",
    "    return suiv.astype(int)\n",
    "\n",
    "\n",
    "FRED_SERIES = {\n",
    "    # energy (daily)\n",
    "    \"Crude Oil (WTI)\": \"DCOILWTICO\",        # WTI (USD/baril)\n",
    "    \"Brent Oil\": \"DCOILBRENTEU\",            # Brent (USD/baril)\n",
    "    \"Natural Gas (Henry Hub)\": \"DHHNGSP\",   # Gaz naturel spot US (USD/MMBtu)\n",
    "\n",
    "    # metal (IMF, monthly)\n",
    "    \"Copper\": \"PCOPPUSDM\",\n",
    "    \"Aluminum\": \"PALUMUSDM\",\n",
    "    \"Nickel\": \"PNICKUSDM\",\n",
    "    \"Zinc\": \"PZINCUSDM\",\n",
    "    \"Tin\": \"PTINUSDM\",\n",
    "    \"Lead\": \"PLEADUSDM\",\n",
    "    \"Iron Ore\": \"PIORECRUSDM\",\n",
    "\n",
    "    # Agriculture (IMF, monthly)\n",
    "    \"Wheat\": \"PWHEAMTUSDM\",\n",
    "    \"Corn\": \"PMAIZMTUSDM\",\n",
    "    \"Soybeans\": \"PSOYBUSDM\",\n",
    "    \"Rice (Thailand)\": \"PRICENPQUSDM\",\n",
    "    \"Coffee (Arabica - Other Mild)\": \"PCOFFOTMUSDM\",\n",
    "    \"Cocoa\": \"PCOCOUSDM\",\n",
    "    \"Sugar No.11 (World)\": \"PSUGAISAUSDM\",\n",
    "    \"Cotton\": \"PCOTTINDUSDM\",\n",
    "}\n",
    "def fetch_fred_monthly(series_dict,start=\"2020-12-31\", end=\"2025-09-30\"):\n",
    "    all_data = []\n",
    "    for name, code in series_dict.items():\n",
    "        # daily\n",
    "        s = fred.get_series(code, observation_start=start, observation_end=end)\n",
    "        s = s.to_frame(name=name)\n",
    "\n",
    "        # end of month\n",
    "        s = s.resample(\"M\").last()\n",
    "\n",
    "        all_data.append(s)\n",
    "\n",
    "    # Concat\n",
    "    df = pd.concat(all_data, axis=1)\n",
    "\n",
    "    # Add month_id\n",
    "\n",
    "    df[\"month_id\"] = df.index.strftime(\"%Y%m\")\n",
    "    cols = [\"month_id\"] + [c for c in df.columns if c != \"month_id\"]\n",
    "    df = df[cols].reset_index(drop=True)\n",
    "    df[\"month_id_join\"] = _suiv_month_id(df[\"month_id\"])\n",
    "    df = df.drop(columns=[\"month_id\"])\n",
    "\n",
    "    return df\n",
    "matieres_premiere= fetch_fred_monthly(FRED_SERIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrSGQfws895B"
   },
   "outputs": [],
   "source": [
    "\n",
    "USA_export_2022=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2022_export_Additional.csv\")\n",
    "USA_import_2022=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2022_import_Additional.csv\")\n",
    "USA_export_2021=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2021_export_Additional.csv\")\n",
    "USA_import_2021=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2021_import_Additional.csv\")\n",
    "USA_export_2023=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2023_exports_Additional.csv\")\n",
    "USA_import_2023=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2023_imports_Additional.csv\")\n",
    "USA_export_2024=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2024_exports_Additional.csv\")\n",
    "USA_import_2024=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2024_imports_Additional.csv\")\n",
    "USA_export_2025=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2025_export_Additional.csv\")\n",
    "USA_import_2025=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/USA_2025_import_Additional.csv\")\n",
    "\n",
    "china_export_2022=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2022_export_Additional.csv\")\n",
    "china_import_2022=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2022_import_Additional.csv\")\n",
    "china_export_2021=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2021_export_Additional.csv\")\n",
    "china_import_2021=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2021_import_Additional.csv\")\n",
    "china_export_2023=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2023_exports_Additional.csv\")\n",
    "china_import_2023=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2023_imports_Additional.csv\")\n",
    "china_export_2024=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2024_exports_Additional.csv\")\n",
    "china_import_2024=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/china_2024_imports_Additional.csv\")\n",
    "china_import_2025=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/China_2025_import_Additional.csv\")\n",
    "china_export_2025=pd.read_csv(\"/content/drive/MyDrive/ai4trade_cf/pre_processed_final/China_2025_export_Additional.csv\")\n",
    "\n",
    "china_import_2025['trade_flow_name'] = 'Import'\n",
    "china_export_2025['trade_flow_name'] = 'Export'\n",
    "USA_all=pd.concat([USA_export_2021, USA_import_2021,USA_export_2022, USA_import_2022,USA_export_2023, USA_import_2023,USA_export_2024, USA_import_2024,USA_export_2025, USA_import_2025], ignore_index=True)\n",
    "china_all=pd.concat([china_export_2021,china_import_2021,china_export_2022,china_import_2022,china_export_2023,china_import_2023,china_export_2024,china_import_2024,china_import_2025,china_export_2025], ignore_index=True)\n",
    "\n",
    "USA_all['product_id_hs4'] = USA_all['product_id_hs4'].astype(str).str.zfill(4)\n",
    "china_all['product_id_hs4'] = china_all['product_id_hs4'].astype(str).str.zfill(4)\n",
    "\n",
    "USA_all= USA_all.drop_duplicates().reset_index(drop=True)\n",
    "china_all= china_all.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "USA_all   = USA_all.loc[~USA_all[\"country_id\"].isin([\"W00\", \"S19\"])]\n",
    "china_all = china_all.loc[~china_all[\"country_id\"].isin([\"W00\", \"S19\"])]\n",
    "# 1. sum of trade vakues\n",
    "USA_sum = USA_all.groupby([\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"], as_index=False)[\"trade_value\"].sum()\n",
    "\n",
    "# 2. Top 30\n",
    "top_USA_sum = (\n",
    "    USA_sum.groupby([\"month_id\",\"trade_flow_name\"])\n",
    "          .apply(lambda x: x.nlargest(30, \"trade_value\"))\n",
    "          .reset_index(drop=True)[[\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"]]\n",
    ")\n",
    "\n",
    "# 3. Subset\n",
    "USA= USA_all.merge(top_USA_sum, on=[\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"], how=\"inner\")\n",
    "\n",
    "# 1. sum of trade vakues\n",
    "china_sum =china_all.groupby([\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"], as_index=False)[\"trade_value\"].sum()\n",
    "\n",
    "# 2. Top 30\n",
    "top_china_sum = (\n",
    "    china_sum.groupby([\"month_id\",\"trade_flow_name\"])\n",
    "          .apply(lambda x: x.nlargest(30, \"trade_value\"))\n",
    "          .reset_index(drop=True)[[\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"]]\n",
    ")\n",
    "\n",
    "# 3. Subset\n",
    "china= china_all.merge(top_china_sum, on=[\"month_id\", \"country_name\", \"country_id\",\"trade_flow_name\"], how=\"inner\")\n",
    "USA = USA.drop(['quantity','nb_product','country_name','product_name_hs4'], axis=1)\n",
    "china = china.drop(['quantity','nb_product','country_name','product_name_hs4'], axis=1)\n",
    "\n",
    "\n",
    "china=china.merge(matieres_premiere, left_on=\"month_id\",right_on=\"month_id_join\", how=\"left\").drop(columns=\"month_id_join\")\n",
    "USA=USA.merge(matieres_premiere,  left_on=\"month_id\",right_on=\"month_id_join\", how=\"left\").drop(columns=\"month_id_join\")\n",
    "\n",
    "# creation of  202510\n",
    "USA_202510_index = USA[['country_id','trade_flow_name','product_id_hs4']].drop_duplicates()\n",
    "USA_202510_index['month_id'] = 202510\n",
    "\n",
    "USA = pd.concat([USA, USA_202510_index], ignore_index=True)\n",
    "\n",
    "\n",
    "china_202510_index = china[['country_id','trade_flow_name','product_id_hs4']].drop_duplicates()\n",
    "china_202510_index['month_id'] = 202510\n",
    "\n",
    "china = pd.concat([china, china_202510_index], ignore_index=True)\n",
    "\n",
    "# corrections\n",
    "flow_map = {\n",
    "    'Exports': 'Export',\n",
    "    'Export': 'Export',\n",
    "    'Imports': 'Import',\n",
    "    'Import': 'Import'\n",
    "}\n",
    "\n",
    "# normalisation\n",
    "USA['trade_flow_name'] = USA['trade_flow_name'].replace(flow_map)\n",
    "USA['country_id'] = USA['country_id'].str.upper()\n",
    "\n",
    "\n",
    "\n",
    "china['trade_flow_name'] = china['trade_flow_name'].replace(flow_map)\n",
    "china['country_id'] = china['country_id'].str.upper()\n",
    "\n",
    "USA= USA.drop_duplicates().reset_index(drop=True)\n",
    "china= china.drop_duplicates().reset_index(drop=True)\n",
    "del  USA_202510_index,china_202510_index,  USA_export_2022, USA_import_2022,USA_export_2021,USA_import_2021,USA_export_2023,USA_import_2023,USA_export_2024,USA_import_2024,USA_export_2025,USA_import_2025,china_export_2022,china_import_2022,china_export_2021,china_export_2023,china_import_2023,china_import_2024,top_china_sum,china_sum,top_USA_sum,china_export_2024,china_all,USA_sum,USA_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z139JMV2t20"
   },
   "source": [
    "# data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2533,
     "status": "ok",
     "timestamp": 1761919568061,
     "user": {
      "displayName": "cheikh fall",
      "userId": "01588746123551831690"
     },
     "user_tz": -60
    },
    "id": "2zuXRd1G391K",
    "outputId": "378038fc-9d42-48a9-b0f3-25e80b9ec4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [country_id, product_id_hs4, trade_flow_name, month_id, count]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [country_id, product_id_hs4, trade_flow_name, month_id, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "china_duplicates_count = (china.groupby(['country_id','product_id_hs4','trade_flow_name','month_id'])\n",
    "                           .size()\n",
    "                           .reset_index(name='count')\n",
    "                           .query('count > 1')\n",
    "                        )\n",
    "\n",
    "print(china_duplicates_count)\n",
    "\n",
    "usa_duplicates_count = (USA.groupby(['country_id','product_id_hs4','trade_flow_name','month_id'])\n",
    "                           .size()\n",
    "                           .reset_index(name='count')\n",
    "                           .query('count > 1')\n",
    "                        )\n",
    "\n",
    "print(usa_duplicates_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1761919575562,
     "user": {
      "displayName": "cheikh fall",
      "userId": "01588746123551831690"
     },
     "user_tz": -60
    },
    "id": "Bnj0ZFwNigT5",
    "outputId": "5662178c-cf73-4a4d-b348-072715439ab7"
   },
   "outputs": [],
   "source": [
    "print(china['month_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1761919599815,
     "user": {
      "displayName": "cheikh fall",
      "userId": "01588746123551831690"
     },
     "user_tz": -60
    },
    "id": "WUMWDzX1l1sz",
    "outputId": "b12b3726-b738-4e67-d485-ecdd10f72e37"
   },
   "outputs": [],
   "source": [
    "print(USA['month_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Os-hbReH24R"
   },
   "source": [
    "#SABLE-12 — Seasonal Anchor with Boosted Lagged Exogènes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFDvQL2Dx9OB"
   },
   "source": [
    "#Amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1761929206649,
     "user": {
      "displayName": "cheikh fall",
      "userId": "01588746123551831690"
     },
     "user_tz": -60
    },
    "id": "4qaFrVxi7xe4",
    "outputId": "7ad6ebc0-cc0e-4539-e909-01f5d2504a43"
   },
   "outputs": [],
   "source": [
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZPlASMG0DiK"
   },
   "source": [
    "# 🇺🇸 Full Mathematical Description of the Forecast Model\n",
    "\n",
    "This document provides a **comprehensive mathematical explanation** of a forecasting model for **monthly trade values** by `(product × country × flow)`.\n",
    "\n",
    "The model combines **naive baselines**, **seasonal and trend decomposition**, **lagged exogenous variables**, and a **robust Huber regression correction**, with **special handling for short or rare series**.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Notation\n",
    "\n",
    "Let each series $s = (\\text{product}, \\text{country}, \\text{flow})$ be observed monthly as:\n",
    "\n",
    "$$\n",
    "\\{ (t_1, y_1), (t_2, y_2), ..., (t_T, y_T) \\}, \\quad y_t \\ge 0\n",
    "$$\n",
    "\n",
    "We denote:\n",
    "- $M_t \\in \\{1,\\dots,12\\}$ the **month** of $t$  \n",
    "- $b_t$ the **base forecast**  \n",
    "- $\\hat y_t$ the **final prediction**  \n",
    "- $x_t$ exogenous variables, possibly lagged\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Base Forecast Candidates\n",
    "\n",
    "For each series, several naive forecast candidates are computed:\n",
    "\n",
    "1. **Last observed value**\n",
    "$$\n",
    "\\text{last} = y_T\n",
    "$$\n",
    "\n",
    "2. **Same month last year**\n",
    "$$\n",
    "\\text{seas12} = y_{T-12}, \\quad \\text{if available}\n",
    "$$\n",
    "\n",
    "3. **Moving averages**\n",
    "$$\n",
    "\\text{ma3} = \\frac{1}{3} \\sum_{i=T-2}^{T} y_i, \\quad\n",
    "\\text{ma6} = \\frac{1}{6} \\sum_{i=T-5}^{T} y_i\n",
    "$$\n",
    "\n",
    "4. **Drift extrapolation**\n",
    "- Compute log-transformed slope over last 6 months:\n",
    "$$\n",
    "\\beta = \\frac{\\sum_{i=T-5}^{T} (i-\\bar i) (\\log(1+y_i) - \\overline{\\log(1+y)} )}{\\sum_{i=T-5}^{T} (i-\\bar i)^2}\n",
    "$$\n",
    "- Extrapolated forecast:\n",
    "$$\n",
    "\\text{drift_last} = y_T \\cdot e^\\beta\n",
    "$$\n",
    "\n",
    "5. **Seasonal-trend forecast** (see Section 2)\n",
    "\n",
    "6. **Same-month median of last 3 years**\n",
    "$$\n",
    "\\text{same_month_med3} = \\text{median}\\{y_t : M_t = M_T, t \\in \\text{last 3 years}\\}\n",
    "$$\n",
    "\n",
    "**Base forecast** $b_{T+1}$ is the median of all valid candidates:\n",
    "\n",
    "$$\n",
    "b_{T+1} = \\text{median}\\{\\text{last}, \\text{seas12}, \\text{ma3}, \\text{ma6}, \\text{drift_last}, \\text{seasonal_ST}, \\text{same_month_med3}\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Seasonal + Trend Decomposition\n",
    "\n",
    "Assume a multiplicative model:\n",
    "\n",
    "$$\n",
    "y_t \\approx g \\cdot S_{M_t} \\cdot \\text{trend}_t\n",
    "$$\n",
    "\n",
    "### 2.1 Seasonal factor $S_m$\n",
    "\n",
    "- Compute global scale $g$ as median of positive $y_t$ values:\n",
    "$$\n",
    "g = \\text{median} \\{ y_t : y_t > 0 \\}\n",
    "$$\n",
    "- For each month $m$:\n",
    "$$\n",
    "S_m = \\text{clip}\\left( \\frac{\\text{median}\\{y_t : M_t = m, y_t>0\\}}{g}, 0.2, 5.0 \\right)\n",
    "$$\n",
    "\n",
    "### 2.2 Deseasonalize and slope estimation\n",
    "\n",
    "- Deseasonalized series:\n",
    "$$\n",
    "d_t = \\frac{y_t}{S_{M_t}}\n",
    "$$\n",
    "- Slope on log-space (small window $w$):\n",
    "$$\n",
    "\\beta = \\frac{\\sum_{i=T-w+1}^{T} (i-\\bar i) (\\log(1 + d_i) - \\overline{\\log(1+d)})}{\\sum_{i=T-w+1}^{T} (i-\\bar i)^2}\n",
    "$$\n",
    "\n",
    "### 2.3 Forecast next month\n",
    "\n",
    "$$\n",
    "\\hat d_{T+1} = d_T \\cdot e^\\beta, \\quad\n",
    "\\text{seasonal_ST} = \\hat d_{T+1} \\cdot S_{M_{T+1}}\n",
    "$$\n",
    "\n",
    "- Blend with same-month median and last value:\n",
    "$$\n",
    "\\text{seasonal_ST} = \\text{median}\\{\\text{seasonal_ST}, y_T, \\text{same_month_med3}\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Exogenous Variable Selection\n",
    "\n",
    "For candidate numeric exogenous variables $x_j$:\n",
    "\n",
    "1. For each lag $L \\in \\{1,3,6\\}$:\n",
    "   - Form pairs $(x_{t-L}, y_t)$ over all series\n",
    "2. Compute Spearman correlation $\\rho_{x_j, L}$\n",
    "3. Keep top `$k$` pairs $(x,L)$ with highest $|\\rho_{x,L}|$\n",
    "\n",
    "These features are used in the Huber regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Construction\n",
    "\n",
    "For each series and target month:\n",
    "\n",
    "| Feature | Formula / Description |\n",
    "|:--|:--|\n",
    "| Month | $M_{T+1}$ |\n",
    "| Binary month indicators | `is_jan`, `is_dec` |\n",
    "| Encodings | Mean target per product/country/flow: $\\text{enc\\_prod} = \\text{mean}(y)$ for that product |\n",
    "| Seasonality | $S_{M_{T+1}}$ |\n",
    "| Lags | $y_T$, $y_{T-1}$, $y_{T-2}$, $y_{T-3}$, $y_{T-6}$, $y_{T-12}$ |\n",
    "| Exogenous | Selected top-$k$ lagged $x_j$ |\n",
    "| Optional: log slope | $\\beta$ from deseasonalized last $w$ points |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Robust Correction: Huber Regression\n",
    "\n",
    "### 5.1 Target transformation\n",
    "\n",
    "Define relative correction in log-space:\n",
    "\n",
    "$$\n",
    "z_t = \\log(1 + y_t) - \\log(1 + b_t)\n",
    "$$\n",
    "\n",
    "- $b_t$ is base forecast\n",
    "- $z_t$ is the **residual correction**\n",
    "\n",
    "### 5.2 Model training\n",
    "\n",
    "- Stack all series and backtests\n",
    "- Train **HuberRegressor** to predict $z_t$ from features $X_t$:\n",
    "\n",
    "$$\n",
    "\\hat z_{T+1} = f_{\\text{Huber}}(X_{T+1})\n",
    "$$\n",
    "\n",
    "Huber loss:\n",
    "$$\n",
    "L_\\delta(r) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} r^2 & |r| \\le \\delta \\\\\n",
    "\\delta (|r| - \\frac{1}{2}\\delta) & |r| > \\delta\n",
    "\\end{cases}, \\quad r = z_t - \\hat z_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Final Forecast\n",
    "\n",
    "- Log-space corrected forecast:\n",
    "$$\n",
    "\\hat y_{\\text{corr}} = \\exp(\\log(1+b_{T+1}) + \\hat z_{T+1}) - 1\n",
    "$$\n",
    "\n",
    "- For rare series (mostly zeros):\n",
    "$$\n",
    "y_{\\text{corr}} = \\max(y_{\\text{floor}}, \\min(\\hat y_{\\text{corr}}, y_{T} \\cdot g_{\\text{cap}}))\n",
    "$$\n",
    "\n",
    "- Weighted blend with base forecast:\n",
    "$$\n",
    "\\hat y_{T+1} = w \\cdot y_{\\text{corr}} + (1-w) \\cdot b_{T+1}, \\quad w = 0.6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Rare Series Handling\n",
    "\n",
    "A series is considered rare if fraction of nonzero $y_t$:\n",
    "\n",
    "$$\n",
    "\\frac{\\#\\{y_t>0\\}}{T} < f_{\\text{rare}}\n",
    "$$\n",
    "\n",
    "- Floor applied:\n",
    "$$\n",
    "y_{\\text{floor}} = \\text{median of last nonzero k values} \\cdot f_{\\text{floor}}\n",
    "$$\n",
    "- Cap growth:\n",
    "$$\n",
    "y_{\\text{corr}} \\le y_T \\cdot g_{\\text{cap}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Evaluation: Micro sMAPE\n",
    "\n",
    "Global metric:\n",
    "$$\n",
    "\\text{sMAPE} = 100 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\frac{2 |y_i - \\hat y_i|}{|y_i| + |\\hat y_i| + \\varepsilon}\n",
    "$$\n",
    "\n",
    "Top-20 metric: only top 20 countries per `(product × flow)` by **sum of trade_value**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary of Model Flow\n",
    "\n",
    "1. **Split train/test**  \n",
    "2. **Identify series** `(product × country × flow)`  \n",
    "3. **Compute naive candidates** (last, MA, drift, seasonal, same-month median)  \n",
    "4. **Compute base forecast** $b$ = median(candidates)  \n",
    "5. **Compute seasonal-trend factors** $S_m$ and slope $\\beta$  \n",
    "6. **Select exogenous features** by Spearman correlation  \n",
    "7. **Construct features**: lags, seasonality, encodings, exog  \n",
    "8. **Backtest and fit Huber regression** on $\\log$ correction  \n",
    "9. **Predict next month**: apply Huber correction in log-space  \n",
    "10. **Rare series adjustment**: floor & growth cap  \n",
    "11. **Blend with base forecast** for final prediction  \n",
    "12. **Evaluate with sMAPE** (global and top-20)\n",
    "\n",
    "---\n",
    "\n",
    "This approach ensures **robustness, interpretability, and high accuracy**, combining **time series heuristics** with **machine learning residual correction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrOjTUP80V5e"
   },
   "source": [
    "\n",
    "\n",
    "# code d'entrainement  des modèles et clalcul de SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJGSFuQe25H3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from scipy.stats import spearmanr\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    prod_col: str = \"product_id_hs4\"\n",
    "    cntry_col: str = \"country_id\"\n",
    "    flow_col: str = \"trade_flow_name\"\n",
    "    time_col: str = \"month_id\"\n",
    "    target: str   = \"trade_value\"\n",
    "\n",
    "    train_start: int = 202101\n",
    "    train_end:   int = 202102\n",
    "    test_month:  int = 202103\n",
    "\n",
    "    # series rules\n",
    "    min_context: int = 13\n",
    "    short_series_max: int = 6\n",
    "    eps_base: float = 1.0\n",
    "\n",
    "    # scarse data\n",
    "    rare_nonzero_frac: float = 0.30\n",
    "    growth_cap_rare: float = 3.0\n",
    "    floor_frac_rare: float = 0.2\n",
    "\n",
    "    # local backtest to train huber\n",
    "    backtest_k: int = 3\n",
    "    random_state: int = 123\n",
    "\n",
    "    # Export\n",
    "    out_xlsx: str = \"USA_forcast.xlsx\"\n",
    "\n",
    "    # EXOGen selection\n",
    "    exog_lags: tuple = (1, 3, 6)\n",
    "    top_exog_k: int = 8\n",
    "    exog_cols: list = None\n",
    "\n",
    "# ==================== UTILS (inchangés) ====================\n",
    "def smape_micro(yt, yp, eps=1e-8):\n",
    "    yt, yp = np.asarray(yt, float), np.asarray(yp, float)\n",
    "    den = np.abs(yt) + np.abs(yp); m = den > 0\n",
    "    return float(100*np.mean(2*np.abs(yp-yt)[m]/(den[m]+eps))) if m.any() else np.nan\n",
    "\n",
    "def slope_ols(arr: np.ndarray):\n",
    "    n = arr.size\n",
    "    if n == 0 or np.any(~np.isfinite(arr)): return 0.0\n",
    "    x = np.arange(n, dtype=float)\n",
    "    x_mean, y_mean = (n - 1) / 2.0, np.nanmean(arr)\n",
    "    num = np.dot(x - x_mean, arr - y_mean)\n",
    "    den = np.dot(x - x_mean, x - x_mean)\n",
    "    return float(num/den) if den > 0 else 0.0\n",
    "\n",
    "def month_of(yyyymm:int)->int:\n",
    "    return int(str(int(yyyymm))[-2:])\n",
    "\n",
    "def is_rare(yhist: pd.Series, min_len=12, frac_thresh=0.30):\n",
    "    yv = pd.to_numeric(yhist, errors=\"coerce\").fillna(0.0).values\n",
    "    if len(yv) < min_len:\n",
    "        return (yv > 0).mean() < frac_thresh\n",
    "    return (yv > 0).mean() < frac_thresh\n",
    "\n",
    "def robust_nonzero_stat(yhist: pd.Series, kind=\"median\", k=5):\n",
    "    yv = pd.to_numeric(yhist, errors=\"coerce\")\n",
    "    nz = yv[yv > 0]\n",
    "    if nz.empty: return 0.0\n",
    "    lastk = nz.tail(min(k, len(nz)))\n",
    "    return float(lastk.median() if kind==\"median\" else lastk.mean())\n",
    "\n",
    "def seasonal_factors_by_month(y: pd.Series, months: pd.Series):\n",
    "    y = pd.to_numeric(y, errors=\"coerce\")\n",
    "    nz = y[y > 0]\n",
    "    gmu = float(nz.median()) if not nz.empty else float(y.replace(0, np.nan).median())\n",
    "    if not np.isfinite(gmu) or gmu <= 0:\n",
    "        gmu = float((y.replace(0, np.nan)).mean() or 1.0)\n",
    "    fac = {}\n",
    "    for m in range(1,13):\n",
    "        ym = y[(months==m) & (y>0)]\n",
    "        medm = float(ym.median()) if not ym.empty else np.nan\n",
    "        fac[m] = float(np.clip((medm/gmu) if np.isfinite(medm) and gmu>0 else 1.0, 0.2, 5.0))\n",
    "    return fac, gmu\n",
    "\n",
    "def same_month_last_k(y: pd.Series, months: pd.Series, target_month:int, k:int=3):\n",
    "    mask = (months == target_month)\n",
    "    vals = pd.to_numeric(y[mask], errors=\"coerce\").dropna()\n",
    "    if vals.empty: return np.nan\n",
    "    return float(vals.tail(k).median())\n",
    "\n",
    "def seasonal_trend_forecast(y: pd.Series, months: pd.Series, target_month:int):\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").fillna(0.0)\n",
    "    if len(y) == 0: return 0.0\n",
    "    S, _ = seasonal_factors_by_month(y, months)\n",
    "    S_series = months.map(S).astype(float).replace(0, 1.0)\n",
    "    de = y / S_series\n",
    "    w = 24 if len(de) >= 24 else (12 if len(de) >= 12 else (6 if len(de) >= 6 else len(de)))\n",
    "    tail = de.tail(w).replace(0, np.nan).dropna()\n",
    "    if tail.empty:\n",
    "        level = float(de.iloc[-1]) if len(de)>0 else 0.0\n",
    "        slope = 0.0\n",
    "    else:\n",
    "        level = float(de.iloc[-1])\n",
    "        slope = slope_ols(np.log1p(tail.values + 1.0))\n",
    "    de_next = level * np.exp(slope)\n",
    "    y_next = de_next * S.get(target_month, 1.0)\n",
    "    samem = same_month_last_k(y, months, target_month, k=3)\n",
    "    if np.isfinite(samem):\n",
    "        y_next = np.median([y_next, samem, float(y.iloc[-1])])\n",
    "    return float(max(0.0, y_next))\n",
    "\n",
    "def naive_candidates(yhist: pd.Series, months: pd.Series, target_month:int):\n",
    "    y = pd.to_numeric(yhist, errors=\"coerce\")\n",
    "    cands = {}\n",
    "    if len(y) >= 1 and pd.notna(y.iloc[-1]):   cands[\"last\"] = float(y.iloc[-1])\n",
    "    if len(y) >= 12 and pd.notna(y.iloc[-12]): cands[\"seas12\"] = float(y.iloc[-12])\n",
    "    if len(y) >= 3:  cands[\"ma3\"] = float(y.iloc[-3:].mean())\n",
    "    if len(y) >= 6:  cands[\"ma6\"] = float(y.iloc[-6:].mean())\n",
    "    if len(y) >= 6 and \"last\" in cands:\n",
    "        drift = slope_ols(np.log1p(y.iloc[-6:].clip(lower=0).values + 1.0))\n",
    "        cands[\"drift_last\"] = float(cands[\"last\"] * np.exp(drift))\n",
    "    cands[\"seasonal_ST\"] = seasonal_trend_forecast(y, months, target_month)\n",
    "    sm = same_month_last_k(y, months, target_month, k=3)\n",
    "    if np.isfinite(sm): cands[\"same_month_med3\"] = float(sm)\n",
    "    return cands\n",
    "\n",
    "def blend_base_from_candidates(cands: dict):\n",
    "    vals = [v for v in cands.values() if np.isfinite(v)]\n",
    "    return float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "def fit_huber_fast(X, y):\n",
    "    if len(y) < 20:\n",
    "        return None\n",
    "    model = HuberRegressor(alpha=1e-3, epsilon=1.35, max_iter=200)\n",
    "    model.fit(np.asarray(X), np.asarray(y))\n",
    "    return model\n",
    "\n",
    "# ----------------- EXOG -----------------\n",
    "def detect_exog_cols(df: pd.DataFrame, cfg: Config):\n",
    "    forbidden = {cfg.time_col, cfg.flow_col, cfg.cntry_col, cfg.prod_col, cfg.target}\n",
    "    cand = [c for c in df.columns if c not in forbidden]\n",
    "    num_cand = []\n",
    "    for c in cand:\n",
    "        try:\n",
    "            s = pd.to_numeric(df[c].dropna().iloc[:1000], errors=\"coerce\")\n",
    "            if s.dropna().size > 0:\n",
    "                num_cand.append(c)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return num_cand\n",
    "\n",
    "def select_top_exog_lag_pairs(train_df: pd.DataFrame, cfg: Config, series_col=\"series_id\"):\n",
    "    exog_cands = cfg.exog_cols or detect_exog_cols(train_df, cfg)\n",
    "    lags = list(cfg.exog_lags)\n",
    "    scores = []\n",
    "    # group by series key in a similar way to the rest of the code (product__country__flow)\n",
    "    train_df = train_df.copy()\n",
    "    train_df[\"__series\"] = train_df[cfg.prod_col].astype(str) + \"__\" + train_df[cfg.cntry_col].astype(str) + \"__\" + train_df[cfg.flow_col].astype(str)\n",
    "    grouped = train_df.sort_values([\"__series\", cfg.time_col]).groupby(\"__series\")\n",
    "    for col in exog_cands:\n",
    "        for L in lags:\n",
    "            xs, ys = [], []\n",
    "            for _, g in grouped:\n",
    "                if col not in g.columns:\n",
    "                    continue\n",
    "                gcol = pd.to_numeric(g[col], errors=\"coerce\")\n",
    "                gtarget = pd.to_numeric(g[cfg.target], errors=\"coerce\")\n",
    "                if len(g) <= L:\n",
    "                    continue\n",
    "                ex = gcol.shift(L).iloc[L:].values\n",
    "                tar = gtarget.iloc[L:].values\n",
    "                mask = np.isfinite(ex) & np.isfinite(tar)\n",
    "                if mask.any():\n",
    "                    xs.append(ex[mask])\n",
    "                    ys.append(tar[mask])\n",
    "            if not xs:\n",
    "                continue\n",
    "            x_all = np.concatenate(xs)\n",
    "            y_all = np.concatenate(ys)\n",
    "            if len(x_all) < 10:\n",
    "                continue\n",
    "            try:\n",
    "                corr, _ = spearmanr(x_all, y_all, nan_policy=\"omit\")\n",
    "                if np.isfinite(corr):\n",
    "                    scores.append((abs(float(corr)), col, L))\n",
    "            except Exception:\n",
    "                continue\n",
    "    scores_sorted = sorted(scores, key=lambda t: t[0], reverse=True)\n",
    "    topk = scores_sorted[: cfg.top_exog_k]\n",
    "    selected = [(col, lag) for _, col, lag in topk]\n",
    "    return selected\n",
    "\n",
    "# Global selected exog list (populated in forecast function)\n",
    "SELECTED_EXOG_PAIRS = []\n",
    "\n",
    "# ----------------- PASSE-PARTOUT: ajout safe d'exogènes dans feats ------------\n",
    "def add_exog_feats_to_feats_dict(feats: dict, hist_df: pd.DataFrame):\n",
    "    # uses SELECTED_EXOG_PAIRS global\n",
    "    for col, lag in SELECTED_EXOG_PAIRS:\n",
    "        key = f\"exog_lag_{col}_L{lag}\"\n",
    "        try:\n",
    "            if col in hist_df.columns:\n",
    "                ser = pd.to_numeric(hist_df[col], errors=\"coerce\")\n",
    "                # prefer exact -lag value if available, otherwise last non-na\n",
    "                if len(ser) >= lag and pd.notna(ser.iloc[-lag]):\n",
    "                    val = float(ser.iloc[-lag])\n",
    "                else:\n",
    "                    snotna = ser.dropna()\n",
    "                    val = float(snotna.iloc[-1]) if not snotna.empty else 0.0\n",
    "            else:\n",
    "                val = 0.0\n",
    "        except Exception:\n",
    "            val = 0.0\n",
    "        feats[key] = float(val)\n",
    "    return feats\n",
    "\n",
    "# ==================== TRAINING + PERFORMANCE ====================\n",
    "def forecast_train(data: pd.DataFrame, cfg: Config = Config()):\n",
    "    global mu, enc_prod, enc_cntry, enc_flow, SELECTED_EXOG_PAIRS\n",
    "\n",
    "    PROD, CNTRY, FLOW, TIME, TARGET = cfg.prod_col, cfg.cntry_col, cfg.flow_col, cfg.time_col, cfg.target\n",
    "    SERIES = \"series_id\"\n",
    "    np.random.seed(cfg.random_state)\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    need = {PROD, CNTRY, FLOW, TIME, TARGET}\n",
    "    assert need.issubset(df.columns), f\"Colonnes manquantes: {list(need - set(df.columns))}\"\n",
    "\n",
    "    # types & month_num\n",
    "    for c in (PROD, CNTRY, FLOW): df[c] = df[c].astype(str)\n",
    "    df[TIME] = pd.to_numeric(df[TIME], errors=\"coerce\").astype(int)\n",
    "    df[\"month_num\"] = df[TIME].map(month_of)\n",
    "\n",
    "    # split\n",
    "    train = df[(df[TIME] >= cfg.train_start) & (df[TIME] <= cfg.train_end)].copy()\n",
    "    test  = df[df[TIME] == cfg.test_month].copy()\n",
    "    assert not train.empty and not test.empty, \"Vérifie la présence des mois exigés.\"\n",
    "\n",
    "    # series id\n",
    "    train[SERIES] = train[PROD] + \"__\" + train[CNTRY] + \"__\" + train[FLOW]\n",
    "    test[SERIES]  = test[PROD]  + \"__\" + test[CNTRY]  + \"__\" + test[FLOW]\n",
    "    ids = sorted(set(train[SERIES]) & set(test[SERIES]))\n",
    "    train = train[train[SERIES].isin(ids)].sort_values([SERIES, TIME]).reset_index(drop=True)\n",
    "    test  = test[test[SERIES].isin(ids)].sort_values([SERIES, TIME]).reset_index(drop=True)\n",
    "\n",
    "    # encod\n",
    "    mu = float(train[TARGET].mean())\n",
    "    enc_prod  = train.groupby(PROD)[TARGET].mean().to_dict()\n",
    "    enc_cntry = train.groupby(CNTRY)[TARGET].mean().to_dict()\n",
    "    enc_flow  = train.groupby(FLOW)[TARGET].mean().to_dict()\n",
    "\n",
    "    # detect exog candidates if not provided\n",
    "    if cfg.exog_cols is None:\n",
    "        cfg.exog_cols = detect_exog_cols(df, cfg)\n",
    "\n",
    "    # select top exog (col,lag) using only train\n",
    "    SELECTED_EXOG_PAIRS = select_top_exog_lag_pairs(train, cfg, series_col=SERIES)\n",
    "    print(\"Selected exog (col,lag):\", SELECTED_EXOG_PAIRS)\n",
    "\n",
    "    # Build per_series_hist but now include exog columns so feats function can read them\n",
    "    extra_cols = [c for (c,_) in SELECTED_EXOG_PAIRS if c in train.columns]\n",
    "    per_series_hist = {\n",
    "        sid: g[[TIME, TARGET, PROD, CNTRY, FLOW, \"month_num\"] + extra_cols].sort_values(TIME).reset_index(drop=True)\n",
    "        for sid, g in train.groupby(SERIES)\n",
    "    }\n",
    "\n",
    "    # Backtest short for Huber\n",
    "    rows_X, rows_y, feature_order = [], [], None\n",
    "\n",
    "    for sid, hist in per_series_hist.items():\n",
    "        y = pd.to_numeric(hist[TARGET], errors=\"coerce\")\n",
    "        if len(y) <= cfg.short_series_max:\n",
    "            continue\n",
    "        K = min(cfg.backtest_k, len(y))\n",
    "        for k in range(1, K+1):\n",
    "            yhist = y.iloc[: -k]\n",
    "            mser  = hist[\"month_num\"].iloc[: -k]\n",
    "            base  = blend_base_from_candidates(naive_candidates(yhist, mser, target_month=int(hist.iloc[-k][\"month_num\"])))\n",
    "            if base <= 0 and len(yhist)>0:\n",
    "                base = max(1e-8, float(yhist.iloc[-1]))\n",
    "            prod, cntry, flow = hist.iloc[-k][PROD], hist.iloc[-k][CNTRY], hist.iloc[-k][FLOW]\n",
    "            month = int(hist.iloc[-k][TIME]) if np.isscalar(hist.iloc[-k][TIME]) else int(hist.iloc[-k][TIME].item())\n",
    "\n",
    "            # original features\n",
    "            y_hist_df = hist.iloc[: -k]\n",
    "            S, _ = seasonal_factors_by_month(y_hist_df[TARGET], y_hist_df[\"month_num\"])\n",
    "            feats = {\n",
    "                \"month\": int(str(month)[-2:]),\n",
    "                \"is_jan\": int(str(month).endswith(\"01\")),\n",
    "                \"is_dec\": int(str(month).endswith(\"12\")),\n",
    "                \"enc_prod\":  enc_prod.get(prod, mu),\n",
    "                \"enc_cntry\": enc_cntry.get(cntry, mu),\n",
    "                \"enc_flow\":  enc_flow.get(flow, mu),\n",
    "                \"season_factor\": float(S.get(int(str(month)[-2:]), 1.0)),\n",
    "            }\n",
    "            for L in (1,2,3,6,12):\n",
    "                feats[f\"lag_{L}\"] = float(y.iloc[-L]) if len(y)>=L and pd.notna(y.iloc[-L]) else 0.0\n",
    "\n",
    "            # --- AJOUT: exogènes sélectionnés (valeur last available respecting lag) ---\n",
    "            feats = add_exog_feats_to_feats_dict(feats, y_hist_df)\n",
    "\n",
    "            if feature_order is None:\n",
    "                feature_order = list(feats.keys())\n",
    "            rows_X.append([feats[k] for k in feature_order])\n",
    "            rows_y.append(np.log1p(float(y.iloc[-k]) + cfg.eps_base) - np.log1p(base + cfg.eps_base))\n",
    "\n",
    "    huber = fit_huber_fast(rows_X, rows_y)\n",
    "\n",
    "    # Prediction for test_month\n",
    "    pred_rows = []\n",
    "    last_obs_value_map, last_obs_month_map = {}, {}\n",
    "\n",
    "    # rebuild per_series_hist for full train (now with exog cols)\n",
    "    per_series_hist = {\n",
    "        sid: g[[TIME, TARGET, PROD, CNTRY, FLOW, \"month_num\"] + extra_cols].sort_values(TIME).reset_index(drop=True)\n",
    "        for sid, g in train.groupby(SERIES)\n",
    "    }\n",
    "\n",
    "    for sid, hist in per_series_hist.items():\n",
    "        # last obs safe\n",
    "        last_obs_value = float(pd.to_numeric(hist[TARGET], errors=\"coerce\").dropna().iloc[-1]) if not hist.empty else np.nan\n",
    "        last_obs_month = int(hist[TIME].dropna().iloc[-1]) if not hist.empty else np.nan\n",
    "        last_obs_value_map[sid] = last_obs_value\n",
    "        last_obs_month_map[sid] = last_obs_month\n",
    "\n",
    "        yhist = pd.to_numeric(hist[TARGET], errors=\"coerce\")\n",
    "        mser  = hist[\"month_num\"]\n",
    "\n",
    "        if len(hist) <= cfg.short_series_max:\n",
    "            base = blend_base_from_candidates(naive_candidates(yhist, mser, target_month=month_of(cfg.test_month)))\n",
    "            y_pred = max(0.0, float(base))\n",
    "        else:\n",
    "            cands = naive_candidates(yhist, mser, target_month=month_of(cfg.test_month))\n",
    "            base  = blend_base_from_candidates(cands)\n",
    "            if base <= 0 and len(yhist)>0:\n",
    "                base = max(1e-8, float(yhist.iloc[-1]))\n",
    "\n",
    "            prod, cntry, flow = hist.iloc[-1][PROD], hist.iloc[-1][CNTRY], hist.iloc[-1][FLOW]\n",
    "\n",
    "            # build feats as in backtest but on full hist\n",
    "            S, _ = seasonal_factors_by_month(pd.to_numeric(hist[TARGET], errors=\"coerce\"), hist[\"month_num\"])\n",
    "            feats = {\n",
    "                \"month\": month_of(cfg.test_month),\n",
    "                \"is_jan\": int(str(cfg.test_month).endswith(\"01\")),\n",
    "                \"is_dec\": int(str(cfg.test_month).endswith(\"12\")),\n",
    "                \"enc_prod\":  enc_prod.get(prod, mu),\n",
    "                \"enc_cntry\": enc_cntry.get(cntry, mu),\n",
    "                \"enc_flow\":  enc_flow.get(flow, mu),\n",
    "                \"season_factor\": float(S.get(month_of(cfg.test_month), 1.0)),\n",
    "            }\n",
    "            for L in (1,2,3,6,12):\n",
    "                feats[f\"lag_{L}\"] = float(yhist.iloc[-L]) if len(yhist)>=L and pd.notna(yhist.iloc[-L]) else 0.0\n",
    "\n",
    "            feats = add_exog_feats_to_feats_dict(feats, hist)\n",
    "\n",
    "            Xrow = np.array([[feats[k] for k in feature_order]]) if feature_order else None\n",
    "            delta = float(huber.predict(Xrow)[0]) if (huber is not None and Xrow is not None) else 0.0\n",
    "            y_corr = float(np.expm1(np.log1p(base + cfg.eps_base) + delta))\n",
    "\n",
    "            # rare series handling\n",
    "            if is_rare(yhist, frac_thresh=cfg.rare_nonzero_frac):\n",
    "                est = robust_nonzero_stat(yhist, \"median\", k=5)\n",
    "                floor = max(1e-8, cfg.floor_frac_rare * est)\n",
    "                y_corr = max(y_corr, floor)\n",
    "                if len(yhist) >= 1:\n",
    "                    prev = float(yhist.iloc[-1])\n",
    "                    y_corr = min(y_corr, prev * cfg.growth_cap_rare)\n",
    "\n",
    "            w = 0.6\n",
    "            y_pred = max(0.0, w*y_corr + (1.0 - w)*base)\n",
    "\n",
    "        # de-duplicate test rows for safety (avoid duplicates causing reindex errors)\n",
    "        te_rows = test[test[SERIES]==sid].drop_duplicates(subset=[PROD, CNTRY, FLOW, TIME])\n",
    "        for _, r in te_rows.iterrows():\n",
    "            time_val = int(r[TIME]) if np.isscalar(r[TIME]) else int(r[TIME].item())\n",
    "            pred_rows.append((\n",
    "                time_val, str(r[PROD]), str(r[CNTRY]), str(r[FLOW]),\n",
    "                float(r.get(TARGET, np.nan)), float(y_pred),\n",
    "                float(last_obs_value_map[sid]), int(last_obs_month_map[sid])\n",
    "            ))\n",
    "\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_rows,\n",
    "        columns=[cfg.time_col, cfg.prod_col, cfg.cntry_col, cfg.flow_col, \"y_true\", \"y_pred\", \"last_obs_value\", \"last_obs_month\"]\n",
    "    ).sort_values([cfg.time_col, cfg.prod_col, cfg.cntry_col, cfg.flow_col]).reset_index(drop=True)\n",
    "\n",
    "    # ==== sMAPE GLOBAL ====\n",
    "    if pred_df[\"y_true\"].notna().any():\n",
    "        smape_all = smape_micro(pred_df[\"y_true\"], pred_df[\"y_pred\"])\n",
    "        print(f\"sMAPE (micro) GLOBAL test {cfg.test_month}: {smape_all:.3f}%  | lignes: {pred_df['y_true'].notna().sum()}/{len(pred_df)}\")\n",
    "    else:\n",
    "        smape_all = np.nan\n",
    "\n",
    "    # TOP-20 ===\n",
    "    test_true = df[df[cfg.time_col] == cfg.test_month][[cfg.prod_col, cfg.cntry_col, cfg.flow_col, cfg.time_col, cfg.target]].copy()\n",
    "\n",
    "    agg = (test_true\n",
    "           .groupby([cfg.prod_col, cfg.flow_col, cfg.time_col, cfg.cntry_col], as_index=False)[cfg.target]\n",
    "           .sum()\n",
    "           .rename(columns={cfg.target: \"sum_trade_value\"}))\n",
    "\n",
    "    agg = agg.sort_values([cfg.prod_col, cfg.flow_col, cfg.time_col, \"sum_trade_value\"],\n",
    "                          ascending=[True, True, True, False])\n",
    "    top20 = agg.groupby([cfg.prod_col, cfg.flow_col, cfg.time_col]).head(20)\n",
    "\n",
    "    pred_top = pred_df.merge(\n",
    "        top20[[cfg.prod_col, cfg.cntry_col, cfg.flow_col, cfg.time_col]],\n",
    "        on=[cfg.prod_col, cfg.cntry_col, cfg.flow_col, cfg.time_col],\n",
    "        how=\"inner\"\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # ==== sMAPE TOP-20 ====\n",
    "    if pred_top[\"y_true\"].notna().any():\n",
    "        smape_top20 = smape_micro(pred_top[\"y_true\"], pred_top[\"y_pred\"])\n",
    "        print(f\"sMAPE (micro) TOP-20 par produit×flow×mois {cfg.test_month}: {smape_top20:.3f}%  | lignes: {pred_top['y_true'].notna().sum()}/{len(pred_top)}\")\n",
    "    else:\n",
    "        smape_top20 = np.nan\n",
    "\n",
    "    # ==== Export Excel ====\n",
    "    with pd.ExcelWriter(cfg.out_xlsx, engine=\"xlsxwriter\") as wr:\n",
    "        pred_top.to_excel(wr, sheet_name=\"predictions_top20\", index=False)\n",
    "    print(f\"📁 Fichier Excel sauvegardé (TOP-20 par produit×flow×mois) → {cfg.out_xlsx}\")\n",
    "\n",
    "    return pred_df, pred_top, smape_all, smape_top20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================== test ====================\n",
    "def forecast_test(data: pd.DataFrame, cfg: Config = Config()):\n",
    "    global mu, enc_prod, enc_cntry, enc_flow, SELECTED_EXOG_PAIRS\n",
    "\n",
    "    PROD, CNTRY, FLOW, TIME, TARGET = cfg.prod_col, cfg.cntry_col, cfg.flow_col, cfg.time_col, cfg.target\n",
    "    SERIES = \"series_id\"\n",
    "    np.random.seed(cfg.random_state)\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    need = {PROD, CNTRY, FLOW, TIME, TARGET}\n",
    "    assert need.issubset(df.columns), f\"Colonnes manquantes: {list(need - set(df.columns))}\"\n",
    "\n",
    "    # types & month_num\n",
    "    for c in (PROD, CNTRY, FLOW): df[c] = df[c].astype(str)\n",
    "    df[TIME] = pd.to_numeric(df[TIME], errors=\"coerce\").astype(int)\n",
    "    df[\"month_num\"] = df[TIME].map(month_of)\n",
    "\n",
    "    # split\n",
    "    train = df[(df[TIME] >= cfg.train_start) & (df[TIME] <= cfg.train_end)].copy()\n",
    "    test  = df[df[TIME] == cfg.test_month].copy()\n",
    "    assert not train.empty and not test.empty, \"Vérifie la présence des mois exigés.\"\n",
    "\n",
    "    # series id\n",
    "    train[SERIES] = train[PROD] + \"__\" + train[CNTRY] + \"__\" + train[FLOW]\n",
    "    test[SERIES]  = test[PROD]  + \"__\" + test[CNTRY]  + \"__\" + test[FLOW]\n",
    "    ids = sorted(set(train[SERIES]) & set(test[SERIES]))\n",
    "    train = train[train[SERIES].isin(ids)].sort_values([SERIES, TIME]).reset_index(drop=True)\n",
    "    test  = test[test[SERIES].isin(ids)].sort_values([SERIES, TIME]).reset_index(drop=True)\n",
    "\n",
    "    # mean encod\n",
    "    mu = float(train[TARGET].mean())\n",
    "    enc_prod  = train.groupby(PROD)[TARGET].mean().to_dict()\n",
    "    enc_cntry = train.groupby(CNTRY)[TARGET].mean().to_dict()\n",
    "    enc_flow  = train.groupby(FLOW)[TARGET].mean().to_dict()\n",
    "\n",
    "    # detect exog candidates if not provided\n",
    "    if cfg.exog_cols is None:\n",
    "        cfg.exog_cols = detect_exog_cols(df, cfg)\n",
    "\n",
    "    # select top exog (col,lag) using only train\n",
    "    SELECTED_EXOG_PAIRS = select_top_exog_lag_pairs(train, cfg, series_col=SERIES)\n",
    "    print(\"Selected exog (col,lag):\", SELECTED_EXOG_PAIRS)\n",
    "\n",
    "    # Build per_series_hist but now include exog columns so feats function can read them\n",
    "    extra_cols = [c for (c,_) in SELECTED_EXOG_PAIRS if c in train.columns]\n",
    "    per_series_hist = {\n",
    "        sid: g[[TIME, TARGET, PROD, CNTRY, FLOW, \"month_num\"] + extra_cols].sort_values(TIME).reset_index(drop=True)\n",
    "        for sid, g in train.groupby(SERIES)\n",
    "    }\n",
    "\n",
    "    # Backtest short for Huber\n",
    "    rows_X, rows_y, feature_order = [], [], None\n",
    "\n",
    "    for sid, hist in per_series_hist.items():\n",
    "        y = pd.to_numeric(hist[TARGET], errors=\"coerce\")\n",
    "        if len(y) <= cfg.short_series_max:\n",
    "            continue\n",
    "        K = min(cfg.backtest_k, len(y))\n",
    "        for k in range(1, K+1):\n",
    "            yhist = y.iloc[: -k]\n",
    "            mser  = hist[\"month_num\"].iloc[: -k]\n",
    "            base  = blend_base_from_candidates(naive_candidates(yhist, mser, target_month=int(hist.iloc[-k][\"month_num\"])))\n",
    "            if base <= 0 and len(yhist)>0:\n",
    "                base = max(1e-8, float(yhist.iloc[-1]))\n",
    "            prod, cntry, flow = hist.iloc[-k][PROD], hist.iloc[-k][CNTRY], hist.iloc[-k][FLOW]\n",
    "            month = int(hist.iloc[-k][TIME]) if np.isscalar(hist.iloc[-k][TIME]) else int(hist.iloc[-k][TIME].item())\n",
    "\n",
    "            # original features\n",
    "            y_hist_df = hist.iloc[: -k]\n",
    "            S, _ = seasonal_factors_by_month(y_hist_df[TARGET], y_hist_df[\"month_num\"])\n",
    "            feats = {\n",
    "                \"month\": int(str(month)[-2:]),\n",
    "                \"is_jan\": int(str(month).endswith(\"01\")),\n",
    "                \"is_dec\": int(str(month).endswith(\"12\")),\n",
    "                \"enc_prod\":  enc_prod.get(prod, mu),\n",
    "                \"enc_cntry\": enc_cntry.get(cntry, mu),\n",
    "                \"enc_flow\":  enc_flow.get(flow, mu),\n",
    "                \"season_factor\": float(S.get(int(str(month)[-2:]), 1.0)),\n",
    "            }\n",
    "            for L in (1,2,3,6,12):\n",
    "                feats[f\"lag_{L}\"] = float(y.iloc[-L]) if len(y)>=L and pd.notna(y.iloc[-L]) else 0.0\n",
    "\n",
    "            # --- add exog ---\n",
    "            feats = add_exog_feats_to_feats_dict(feats, y_hist_df)\n",
    "\n",
    "            if feature_order is None:\n",
    "                feature_order = list(feats.keys())\n",
    "            rows_X.append([feats[k] for k in feature_order])\n",
    "            rows_y.append(np.log1p(float(y.iloc[-k]) + cfg.eps_base) - np.log1p(base + cfg.eps_base))\n",
    "\n",
    "    huber = fit_huber_fast(rows_X, rows_y)\n",
    "\n",
    "    # Prediction for test_month\n",
    "    pred_rows = []\n",
    "    last_obs_value_map, last_obs_month_map = {}, {}\n",
    "\n",
    "    # rebuild per_series_hist for full train (now with exog cols)\n",
    "    per_series_hist = {\n",
    "        sid: g[[TIME, TARGET, PROD, CNTRY, FLOW, \"month_num\"] + extra_cols].sort_values(TIME).reset_index(drop=True)\n",
    "        for sid, g in train.groupby(SERIES)\n",
    "    }\n",
    "\n",
    "    for sid, hist in per_series_hist.items():\n",
    "        # last obs safe\n",
    "        last_obs_value = float(pd.to_numeric(hist[TARGET], errors=\"coerce\").dropna().iloc[-1]) if not hist.empty else np.nan\n",
    "        last_obs_month = int(hist[TIME].dropna().iloc[-1]) if not hist.empty else np.nan\n",
    "        last_obs_value_map[sid] = last_obs_value\n",
    "        last_obs_month_map[sid] = last_obs_month\n",
    "\n",
    "        yhist = pd.to_numeric(hist[TARGET], errors=\"coerce\")\n",
    "        mser  = hist[\"month_num\"]\n",
    "\n",
    "        if len(hist) <= cfg.short_series_max:\n",
    "            base = blend_base_from_candidates(naive_candidates(yhist, mser, target_month=month_of(cfg.test_month)))\n",
    "            y_pred = max(0.0, float(base))\n",
    "        else:\n",
    "            cands = naive_candidates(yhist, mser, target_month=month_of(cfg.test_month))\n",
    "            base  = blend_base_from_candidates(cands)\n",
    "            if base <= 0 and len(yhist)>0:\n",
    "                base = max(1e-8, float(yhist.iloc[-1]))\n",
    "\n",
    "            prod, cntry, flow = hist.iloc[-1][PROD], hist.iloc[-1][CNTRY], hist.iloc[-1][FLOW]\n",
    "\n",
    "            # build feats as in backtest but on full hist\n",
    "            S, _ = seasonal_factors_by_month(pd.to_numeric(hist[TARGET], errors=\"coerce\"), hist[\"month_num\"])\n",
    "            feats = {\n",
    "                \"month\": month_of(cfg.test_month),\n",
    "                \"is_jan\": int(str(cfg.test_month).endswith(\"01\")),\n",
    "                \"is_dec\": int(str(cfg.test_month).endswith(\"12\")),\n",
    "                \"enc_prod\":  enc_prod.get(prod, mu),\n",
    "                \"enc_cntry\": enc_cntry.get(cntry, mu),\n",
    "                \"enc_flow\":  enc_flow.get(flow, mu),\n",
    "                \"season_factor\": float(S.get(month_of(cfg.test_month), 1.0)),\n",
    "            }\n",
    "            for L in (1,2,3,6,12):\n",
    "                feats[f\"lag_{L}\"] = float(yhist.iloc[-L]) if len(yhist)>=L and pd.notna(yhist.iloc[-L]) else 0.0\n",
    "\n",
    "            feats = add_exog_feats_to_feats_dict(feats, hist)\n",
    "\n",
    "            Xrow = np.array([[feats[k] for k in feature_order]]) if feature_order else None\n",
    "            delta = float(huber.predict(Xrow)[0]) if (huber is not None and Xrow is not None) else 0.0\n",
    "            y_corr = float(np.expm1(np.log1p(base + cfg.eps_base) + delta))\n",
    "\n",
    "            # rare series handling\n",
    "            if is_rare(yhist, frac_thresh=cfg.rare_nonzero_frac):\n",
    "                est = robust_nonzero_stat(yhist, \"median\", k=5)\n",
    "                floor = max(1e-8, cfg.floor_frac_rare * est)\n",
    "                y_corr = max(y_corr, floor)\n",
    "                if len(yhist) >= 1:\n",
    "                    prev = float(yhist.iloc[-1])\n",
    "                    y_corr = min(y_corr, prev * cfg.growth_cap_rare)\n",
    "\n",
    "            w = 0.6\n",
    "            y_pred = max(0.0, w*y_corr + (1.0 - w)*base)\n",
    "\n",
    "        # de-duplicate test rows for safety (avoid duplicates causing reindex errors)\n",
    "        te_rows = test[test[SERIES]==sid].drop_duplicates(subset=[PROD, CNTRY, FLOW, TIME])\n",
    "        for _, r in te_rows.iterrows():\n",
    "            time_val = int(r[TIME]) if np.isscalar(r[TIME]) else int(r[TIME].item())\n",
    "            pred_rows.append((\n",
    "                time_val, str(r[PROD]), str(r[CNTRY]), str(r[FLOW]),\n",
    "                 float(y_pred),\n",
    "                float(last_obs_value_map[sid]), int(last_obs_month_map[sid])\n",
    "            ))\n",
    "\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_rows,\n",
    "        columns=[cfg.time_col, cfg.prod_col, cfg.cntry_col, cfg.flow_col,  \"y_pred\", \"last_obs_value\", \"last_obs_month\"]\n",
    "    ).sort_values([cfg.time_col, cfg.prod_col, cfg.cntry_col, cfg.flow_col]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "       # ==== Export Excel ====\n",
    "    with pd.ExcelWriter(cfg.out_xlsx, engine=\"xlsxwriter\") as wr:\n",
    "        pred_df.to_excel(wr, sheet_name=\"predictions_202510\", index=False)\n",
    "    print(f\"pred_tr → {cfg.out_xlsx}\")\n",
    "\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEsnJvnS0rvw"
   },
   "source": [
    "USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeqL9KJr0z5Y"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config_usa:\n",
    "    prod_col: str = \"product_id_hs4\"\n",
    "    cntry_col: str = \"country_id\"\n",
    "    flow_col: str = \"trade_flow_name\"\n",
    "    time_col: str = \"month_id\"\n",
    "    target: str   = \"trade_value\"\n",
    "\n",
    "    train_start: int = 202101\n",
    "    train_end:   int = 202102\n",
    "    test_month:  int = 202103\n",
    "\n",
    "    # series rules\n",
    "    min_context: int = 13\n",
    "    short_series_max: int = 6\n",
    "    eps_base: float = 1.0\n",
    "\n",
    "    # scarse data\n",
    "    rare_nonzero_frac: float = 0.30\n",
    "    growth_cap_rare: float = 3.0\n",
    "    floor_frac_rare: float = 0.2\n",
    "\n",
    "    # local backtest to train huber\n",
    "    backtest_k: int = 3\n",
    "    random_state: int = 123\n",
    "\n",
    "    # Export\n",
    "    out_xlsx: str = \"USA_forcast.xlsx\"\n",
    "\n",
    "    # EXOGen selection\n",
    "    exog_lags: tuple = (1, 3, 6)\n",
    "    top_exog_k: int = 8\n",
    "    exog_cols: list = None\n",
    "\n",
    "\n",
    "#pred_USA= forecast_test(USA\n",
    "pred_USA=forecast_test(data=USA, cfg= Config_usa())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cguuq_bJ43FM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntcTgxhFwvdT"
   },
   "source": [
    "#china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqQKDK2CwxYR"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config_china:\n",
    "    prod_col: str = \"product_id_hs4\"\n",
    "    cntry_col: str = \"country_id\"\n",
    "    flow_col: str = \"trade_flow_name\"\n",
    "    time_col: str = \"month_id\"\n",
    "    target: str   = \"trade_value\"\n",
    "\n",
    "    train_start: int = 202101\n",
    "    train_end:   int = 202509\n",
    "    test_month:  int = 202510\n",
    "\n",
    "    # Règles séries\n",
    "    min_context: int = 13\n",
    "    short_series_max: int = 6\n",
    "    eps_base: float = 1.0\n",
    "\n",
    "    # Séries rares\n",
    "    rare_nonzero_frac: float = 0.30\n",
    "    growth_cap_rare: float = 3.0\n",
    "    floor_frac_rare: float = 0.2\n",
    "\n",
    "    # Petit backtest local pour entraîner Huber\n",
    "    backtest_k: int = 3\n",
    "    random_state: int = 123\n",
    "\n",
    "    # Export\n",
    "    out_xlsx: str = \"china_forcast.xlsx\"\n",
    "\n",
    "    # EXOGènes selection\n",
    "    exog_lags: tuple = (1, 3, 6)\n",
    "    top_exog_k: int = 8\n",
    "    exog_cols: list = None   # si None => auto-detect depuis USA\n",
    "\n",
    "pred_USA=forecast_test(data=china, cfg= Config_china())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Vk4wa6aPL1jLM7b5_ylVbOeUK17zY8cQ",
     "timestamp": 1761936168886
    },
    {
     "file_id": "129JCWp4xc5mScw0OfjlD2wZy-kGQ0MjU",
     "timestamp": 1761928756851
    },
    {
     "file_id": "1ypjOgWx-I2eiodOHEKSJRKmxVpnGXyes",
     "timestamp": 1761783182839
    },
    {
     "file_id": "1ozhiuegYTZBOporkeQjMfPNS1MdwKoYQ",
     "timestamp": 1759326923098
    },
    {
     "file_id": "1-eQ2ucaorghWlzVlyPtFC1jm6CyMfRGV",
     "timestamp": 1759078490369
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
