{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e748536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Directory containing the CSV files\n",
    "input_dir = 'comtrade_monthly_hs4_outputs'\n",
    "output_dir = 'comtrade_monthly_hs4_outputs/merged'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Read CSV with correct encoding and handle missing column names\"\"\"\n",
    "    encodings = ['latin-1', 'iso-8859-1', 'windows-1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # First read to get the header\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                header_line = f.readline().strip()\n",
    "                data_line = f.readline().strip()\n",
    "            \n",
    "            # Count columns in header vs data\n",
    "            header_cols = header_line.count(',') + 1\n",
    "            data_cols = data_line.count(',') + 1\n",
    "            \n",
    "            if data_cols > header_cols:\n",
    "                # Add placeholder names for missing columns\n",
    "                missing_cols = data_cols - header_cols\n",
    "                print(f\"    Adding {missing_cols} placeholder column names\")\n",
    "                \n",
    "                # Read the original header names\n",
    "                df_temp = pd.read_csv(file_path, encoding=encoding, nrows=0)\n",
    "                original_columns = df_temp.columns.tolist()\n",
    "                \n",
    "                # Add placeholder names\n",
    "                new_columns = original_columns + [f'unnamed_col_{i}' for i in range(missing_cols)]\n",
    "                \n",
    "                # Read with explicit column names\n",
    "                df = pd.read_csv(file_path, encoding=encoding, names=new_columns, skiprows=1)\n",
    "            else:\n",
    "                # Read normally\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except (UnicodeDecodeError, Exception) as e:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(f\"Could not read file with any encoding: {file_path}\")\n",
    "\n",
    "# Define the years and trade types\n",
    "years = [2021, 2022, 2025]\n",
    "trade_types = {'M': 'import', 'X': 'export'}\n",
    "\n",
    "# Process each year and trade type\n",
    "for year in years:\n",
    "    for trade_code, trade_name in trade_types.items():\n",
    "        print(f\"\\nProcessing {year} {trade_name}...\")\n",
    "        \n",
    "        # Find all matching files for this year and trade type\n",
    "        pattern = os.path.join(input_dir, f\"USA_{trade_code}_{year}*_HS4.csv\")\n",
    "        files = sorted(glob(pattern))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"  No files found for {year} {trade_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Found {len(files)} files\")\n",
    "        \n",
    "        # Read and concatenate all files\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            try:\n",
    "                df = read_csv_with_encoding(file)\n",
    "                \n",
    "                # Check if 'qty' column exists\n",
    "                if 'qty' not in df.columns:\n",
    "                    print(f\"    ⚠️  'qty' column not found. Available columns: {df.columns.tolist()}\")\n",
    "                    continue\n",
    "                \n",
    "                # Filter rows where qty >= 0\n",
    "                df_filtered = df[df['qty'] >= 0]\n",
    "                dfs.append(df_filtered)\n",
    "                print(f\"    ✓ {os.path.basename(file)}: {len(df)} rows -> {len(df_filtered)} rows after filtering (qty >= 0)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error reading {os.path.basename(file)}: {e}\")\n",
    "        \n",
    "        # Concatenate all dataframes\n",
    "        if dfs:\n",
    "            merged_df = pd.concat(dfs, ignore_index=True)\n",
    "            \n",
    "            # Verify columns are correct\n",
    "            print(f\"\\n  Merged dataframe info:\")\n",
    "            print(f\"    Total rows: {len(merged_df):,}\")\n",
    "            print(f\"    First 5 columns: {merged_df.columns[:5].tolist()}\")\n",
    "            print(f\"    Sample data:\")\n",
    "            print(merged_df[['typeCode', 'freqCode', 'refPeriodId', 'qty']].head())\n",
    "            \n",
    "            # Save merged file\n",
    "            output_file = os.path.join(output_dir, f\"USA_{year}_{trade_name}.csv\")\n",
    "            merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"\\n  ✓✓ SAVED: {output_file}\")\n",
    "        else:\n",
    "            print(f\"  No data to merge for {year} {trade_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All files processed successfully!\")\n",
    "print(f\"Output files saved in: {output_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d9e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29e8c735",
   "metadata": {},
   "source": [
    "## CHINA DATA SPLITTING AND REPAIRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path\n",
    "input_file = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\combined_df.csv'\n",
    "output_dir = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\split'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Try different encodings\"\"\"\n",
    "    encodings = ['latin-1', 'iso-8859-1', 'windows-1252', 'utf-8']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df\n",
    "        except:\n",
    "            continue\n",
    "    raise Exception(\"Could not read file with any encoding\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Reading and fixing China data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read the CSV\n",
    "df = read_csv_with_encoding(input_file)\n",
    "\n",
    "print(f\"\\nOriginal data:\")\n",
    "print(f\"  Total rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  First 5 column names: {df.columns[:5].tolist()}\")\n",
    "print(f\"\\nFirst few rows (before fix):\")\n",
    "print(df.head()[df.columns[:5]])\n",
    "\n",
    "# Fix the column shift issue\n",
    "# Get the column names\n",
    "old_columns = df.columns.tolist()\n",
    "\n",
    "# Shift column names to the right by 1\n",
    "# Drop the first column name (typeCode) and add a placeholder at the end\n",
    "new_columns = old_columns[1:] + ['unnamed_extra_col']\n",
    "\n",
    "# Apply new column names\n",
    "df.columns = new_columns\n",
    "\n",
    "print(f\"\\nAfter fixing column alignment:\")\n",
    "print(f\"  First 5 column names: {df.columns[:5].tolist()}\")\n",
    "print(f\"\\nFirst few rows (after fix):\")\n",
    "print(df.head()[df.columns[:5]])\n",
    "\n",
    "# Check for required columns\n",
    "if 'refYear' not in df.columns or 'flowDesc' not in df.columns:\n",
    "    print(f\"\\n⚠️  Warning: Required columns not found!\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "else:\n",
    "    # Check unique values\n",
    "    print(f\"\\nUnique years: {sorted(df['refYear'].unique())}\")\n",
    "    print(f\"Unique flows: {df['flowDesc'].unique()}\")\n",
    "    \n",
    "    # Check if qty column exists\n",
    "    if 'qty' in df.columns:\n",
    "        print(f\"\\nFiltering by qty > 200...\")\n",
    "        df_filtered = df[df['qty'] > 200].copy()\n",
    "        print(f\"  Rows before filtering: {len(df):,}\")\n",
    "        print(f\"  Rows after filtering: {len(df_filtered):,}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  'qty' column not found. Available columns: {df.columns.tolist()}\")\n",
    "        df_filtered = df.copy()\n",
    "    \n",
    "    # Split by year and flow\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"Splitting data by year and trade flow...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    years = sorted(df_filtered['refYear'].unique())\n",
    "    flows = df_filtered['flowDesc'].unique()\n",
    "    \n",
    "    for year in years:\n",
    "        for flow in flows:\n",
    "            # Filter data\n",
    "            mask = (df_filtered['refYear'] == year) & (df_filtered['flowDesc'] == flow)\n",
    "            df_subset = df_filtered[mask]\n",
    "            \n",
    "            if len(df_subset) > 0:\n",
    "                # Create filename (china_2021_import.csv)\n",
    "                flow_name = flow.lower()\n",
    "                filename = f\"china_{year}_{flow_name}.csv\"\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                \n",
    "                # Save file\n",
    "                df_subset.to_csv(output_path, index=False, encoding='utf-8')\n",
    "                print(f\"✓ Saved: {filename} ({len(df_subset):,} rows)\")\n",
    "            else:\n",
    "                print(f\"  No data for {year} {flow}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"All files saved successfully!\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7d5ba",
   "metadata": {},
   "source": [
    "## RENAMING AND NORMALIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e64417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define directories\n",
    "usa_dir = 'comtrade_monthly_hs4_outputs/merged'\n",
    "china_dir = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\split'\n",
    "\n",
    "# Column mapping: old_name -> new_name\n",
    "column_mapping = {\n",
    "    'period': 'month_id',\n",
    "    'flowDesc': 'trade_flow_name',\n",
    "    'partnerISO': 'country_id',\n",
    "    'partnerDesc': 'country_name',\n",
    "    'cmdCode': 'product_id_hs4',\n",
    "    'primaryValue': 'trade_value',\n",
    "    'qty': 'quantity',\n",
    "    'cmdDesc': 'product_name_hs4'\n",
    "}\n",
    "\n",
    "# Final column order\n",
    "final_columns = ['month_id', 'trade_flow_name', 'country_id', 'country_name', \n",
    "                 'product_id_hs4', 'trade_value', 'quantity', 'nb_product', 'product_name_hs4']\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Try different encodings\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df\n",
    "        except:\n",
    "            continue\n",
    "    raise Exception(f\"Could not read file: {file_path}\")\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"Process a single file: select columns, rename, filter, and save\"\"\"\n",
    "    try:\n",
    "        # Read file\n",
    "        df = read_csv_with_encoding(input_path)\n",
    "        \n",
    "        # Check if all required columns exist\n",
    "        missing_cols = [col for col in column_mapping.keys() if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"  ⚠️  Missing columns: {missing_cols}\")\n",
    "            print(f\"      Available columns: {df.columns.tolist()}\")\n",
    "            return False\n",
    "        \n",
    "        # Select only the columns we need\n",
    "        df_selected = df[list(column_mapping.keys())].copy()\n",
    "        \n",
    "        # Rename columns\n",
    "        df_renamed = df_selected.rename(columns=column_mapping)\n",
    "        \n",
    "        # Convert product_id_hs4 to string\n",
    "        df_renamed['product_id_hs4'] = df_renamed['product_id_hs4'].astype(str)\n",
    "        \n",
    "        # Calculate nb_product: count distinct products per month_id and country_id\n",
    "        print(f\"    Calculating nb_product (distinct products per month/country)...\")\n",
    "        df_renamed['nb_product'] = df_renamed.groupby(['month_id', 'country_id'])['product_id_hs4'].transform('nunique')\n",
    "        \n",
    "        # Filter: keep only rows where nb_product > 200\n",
    "        rows_before = len(df_renamed)\n",
    "        df_filtered = df_renamed[df_renamed['nb_product'] > 200].copy()\n",
    "        rows_after = len(df_filtered)\n",
    "        print(f\"    Filtered nb_product > 200: {rows_before:,} rows → {rows_after:,} rows\")\n",
    "        \n",
    "        # Reorder columns\n",
    "        df_final = df_filtered[final_columns]\n",
    "        \n",
    "        # Save\n",
    "        df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Processing USA files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process USA files\n",
    "usa_files = glob(os.path.join(usa_dir, \"USA_*.csv\"))\n",
    "for file_path in usa_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Skip if already a _final file\n",
    "    if '_final' in filename:\n",
    "        continue\n",
    "    if '_Additional' in filename:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Create output filename\n",
    "    name_without_ext = filename.replace('.csv', '')\n",
    "    output_filename = f\"{name_without_ext}_final.csv\"\n",
    "    output_path = os.path.join(usa_dir, output_filename)\n",
    "    \n",
    "    print(f\"\\n{filename}\")\n",
    "    success = process_file(file_path, output_path)\n",
    "    \n",
    "    if success:\n",
    "        # Check file size\n",
    "        df_check = pd.read_csv(output_path)\n",
    "        print(f\"  ✓ Saved: {output_filename}\")\n",
    "        print(f\"    Final rows: {len(df_check):,}\")\n",
    "        print(f\"    Columns: {df_check.columns.tolist()}\")\n",
    "        print(f\"    nb_product range: {df_check['nb_product'].min()} to {df_check['nb_product'].max()}\")\n",
    "        print(f\"    Sample data:\")\n",
    "        print(df_check[['month_id', 'country_id', 'product_id_hs4', 'nb_product']].head(5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Processing China files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process China files\n",
    "china_files = glob(os.path.join(china_dir, \"china_*.csv\"))\n",
    "for file_path in china_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Skip if already a _final file\n",
    "    if '_final' in filename:\n",
    "        continue\n",
    "    if '_Additional' in filename:\n",
    "        continue\n",
    "\n",
    "    # Create output filename\n",
    "    name_without_ext = filename.replace('.csv', '')\n",
    "    output_filename = f\"{name_without_ext}_final.csv\"\n",
    "    output_path = os.path.join(china_dir, output_filename)\n",
    "    \n",
    "    print(f\"\\n{filename}\")\n",
    "    success = process_file(file_path, output_path)\n",
    "    \n",
    "    if success:\n",
    "        # Check file size\n",
    "        df_check = pd.read_csv(output_path)\n",
    "        print(f\"  ✓ Saved: {output_filename}\")\n",
    "        print(f\"    Final rows: {len(df_check):,}\")\n",
    "        print(f\"    Columns: {df_check.columns.tolist()}\")\n",
    "        print(f\"    nb_product range: {df_check['nb_product'].min()} to {df_check['nb_product'].max()}\")\n",
    "        print(f\"    Sample data:\")\n",
    "        print(df_check[['month_id', 'country_id', 'product_id_hs4', 'nb_product']].head(5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All files processed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dea568",
   "metadata": {},
   "source": [
    "## ADDITIONAL DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define directories and file paths\n",
    "usa_dir = 'comtrade_monthly_hs4_outputs/merged'\n",
    "china_dir = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\split'\n",
    "indicators_file = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\df_long.csv'\n",
    "reer_file = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\outputs\\EER_COUNTRIES.csv'\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Try different encodings\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df\n",
    "        except:\n",
    "            continue\n",
    "    raise Exception(f\"Could not read file: {file_path}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading and preparing indicators data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load indicators data\n",
    "df_indicators = read_csv_with_encoding(indicators_file)\n",
    "print(f\"\\n✓ Loaded indicators data: {len(df_indicators):,} rows\")\n",
    "\n",
    "# Check what indicators are available\n",
    "print(f\"\\nChecking available indicators...\")\n",
    "print(f\"Unique indicators in data:\")\n",
    "for ind in df_indicators['INDICATOR'].unique():\n",
    "    if 'GDP' in ind.upper() or 'GROSS' in ind.upper() or 'CONSUMPTION' in ind.upper() or 'CAPITAL' in ind.upper() or 'INVENTOR' in ind.upper():\n",
    "        count = len(df_indicators[df_indicators['INDICATOR'] == ind])\n",
    "        print(f\"  - {ind}: {count} rows\")\n",
    "\n",
    "# Let's check filters\n",
    "print(f\"\\nUnique PRICE_TYPE: {df_indicators['PRICE_TYPE'].unique()}\")\n",
    "print(f\"Unique S_ADJUSTMENT: {df_indicators['S_ADJUSTMENT'].unique()}\")\n",
    "\n",
    "# Filter indicators - let's be more flexible with GDP name\n",
    "indicator_keywords = ['GDP', 'consumption expenditure', 'capital formation', 'inventories']\n",
    "\n",
    "df_indicators_filtered = df_indicators[\n",
    "    (df_indicators['PRICE_TYPE'] == 'Constant prices') &\n",
    "    (df_indicators['S_ADJUSTMENT'] == 'Seasonally adjusted (SA)') &\n",
    "    (df_indicators['INDICATOR'].apply(lambda x: any(keyword.lower() in x.lower() for keyword in indicator_keywords)))\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n✓ Filtered indicators: {len(df_indicators_filtered):,} rows\")\n",
    "print(f\"Indicators found:\")\n",
    "for ind in df_indicators_filtered['INDICATOR'].unique():\n",
    "    count = len(df_indicators_filtered[df_indicators_filtered['INDICATOR'] == ind])\n",
    "    print(f\"  - {ind}: {count} rows\")\n",
    "\n",
    "# Pivot indicators to create one column per indicator\n",
    "print(f\"\\n  Pivoting indicators to wide format...\")\n",
    "df_indicators_wide = df_indicators_filtered.pivot_table(\n",
    "    index=['ISO3', 'PERIOD'],\n",
    "    columns='INDICATOR',\n",
    "    values='VALUE',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns\n",
    "column_rename = {\n",
    "    'ISO3': 'country_id',\n",
    "    'PERIOD': 'month_id'\n",
    "}\n",
    "\n",
    "# Add renaming for each indicator found\n",
    "for col in df_indicators_wide.columns:\n",
    "    if 'GDP' in col or 'Gross domestic product' in col:\n",
    "        column_rename[col] = 'gdp_constant_sa'\n",
    "    elif 'Final consumption expenditure' in col:\n",
    "        column_rename[col] = 'final_consumption_constant_sa'\n",
    "    elif 'Gross capital formation' in col:\n",
    "        column_rename[col] = 'gross_capital_formation_constant_sa'\n",
    "    elif 'Changes in inventories' in col:\n",
    "        column_rename[col] = 'changes_inventories_constant_sa'\n",
    "\n",
    "df_indicators_wide = df_indicators_wide.rename(columns=column_rename)\n",
    "\n",
    "print(f\"  Pivoted data shape: {df_indicators_wide.shape}\")\n",
    "print(f\"  Columns: {df_indicators_wide.columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Loading and preparing REER data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load REER data\n",
    "df_reer = read_csv_with_encoding(reer_file)\n",
    "print(f\"\\n✓ Loaded REER data: {len(df_reer)} rows\")\n",
    "print(f\"  Columns: {df_reer.columns.tolist()[:15]}...\")  # First 15 columns\n",
    "\n",
    "# Check indicators available\n",
    "print(f\"\\nUnique INDICATOR values:\")\n",
    "for ind in df_reer['INDICATOR'].unique():\n",
    "    print(f\"  - {ind}\")\n",
    "\n",
    "# Filter for REER only\n",
    "df_reer_filtered = df_reer[df_reer['INDICATOR'].str.contains('REER', case=False, na=False)].copy()\n",
    "print(f\"\\n✓ Filtered for REER: {len(df_reer_filtered)} rows\")\n",
    "\n",
    "# Get the date columns (format: 2021-M01, 2021-M02, etc.)\n",
    "date_columns = [col for col in df_reer_filtered.columns if '-M' in col]\n",
    "print(f\"  Found {len(date_columns)} date columns from {date_columns[0]} to {date_columns[-1]}\")\n",
    "\n",
    "# Transform from wide to long format\n",
    "print(f\"\\n  Transforming REER data from wide to long format...\")\n",
    "\n",
    "# Melt the dataframe\n",
    "df_reer_long = df_reer_filtered.melt(\n",
    "    id_vars=['COUNTRY.ID'],\n",
    "    value_vars=date_columns,\n",
    "    var_name='period_str',\n",
    "    value_name='REER'\n",
    ")\n",
    "\n",
    "# Convert period format from \"2021-M01\" to \"202101\"\n",
    "df_reer_long['month_id'] = df_reer_long['period_str'].str.replace('-M', '').astype(int)\n",
    "\n",
    "# Rename COUNTRY.ID to country_id\n",
    "df_reer_long = df_reer_long.rename(columns={'COUNTRY.ID': 'country_id'})\n",
    "\n",
    "# Keep only country_id, month_id, and REER\n",
    "df_reer_final = df_reer_long[['country_id', 'month_id', 'REER']].copy()\n",
    "\n",
    "# Remove rows with missing REER values\n",
    "df_reer_final = df_reer_final.dropna(subset=['REER'])\n",
    "\n",
    "print(f\"  Final REER data shape: {df_reer_final.shape}\")\n",
    "print(f\"  Sample:\")\n",
    "print(df_reer_final.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Processing USA files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process USA files\n",
    "usa_files = glob(os.path.join(usa_dir, \"USA_*_final.csv\"))\n",
    "for file_path in usa_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file\n",
    "        df = read_csv_with_encoding(file_path)\n",
    "        print(f\"  Original rows: {len(df):,}\")\n",
    "        \n",
    "        # Merge with indicators\n",
    "        df_merged = df.merge(\n",
    "            df_indicators_wide,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        indicator_cols = [col for col in df_merged.columns if '_constant_sa' in col]\n",
    "        rows_with_indicators = df_merged[indicator_cols].notna().any(axis=1).sum()\n",
    "        print(f\"  ✓ Added {len(indicator_cols)} indicator columns\")\n",
    "        print(f\"    Rows with at least one indicator: {rows_with_indicators:,}\")\n",
    "        \n",
    "        # Merge with REER\n",
    "        df_merged = df_merged.merge(\n",
    "            df_reer_final,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        rows_with_reer = df_merged['REER'].notna().sum()\n",
    "        print(f\"  ✓ Added REER column\")\n",
    "        print(f\"    Rows with REER data: {rows_with_reer:,}\")\n",
    "        \n",
    "        # Save\n",
    "        output_filename = filename.replace('_final.csv', '_Additional.csv')\n",
    "        output_path = os.path.join(usa_dir, output_filename)\n",
    "        df_merged.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"  ✓ Saved: {output_filename}\")\n",
    "        print(f\"    Total columns: {len(df_merged.columns)}\")\n",
    "        print(f\"    Indicator columns: {indicator_cols}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Processing China files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process China files\n",
    "china_files = glob(os.path.join(china_dir, \"china_*_final.csv\"))\n",
    "for file_path in china_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file\n",
    "        df = read_csv_with_encoding(file_path)\n",
    "        print(f\"  Original rows: {len(df):,}\")\n",
    "        \n",
    "        # Merge with indicators\n",
    "        df_merged = df.merge(\n",
    "            df_indicators_wide,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        indicator_cols = [col for col in df_merged.columns if '_constant_sa' in col]\n",
    "        rows_with_indicators = df_merged[indicator_cols].notna().any(axis=1).sum()\n",
    "        print(f\"  ✓ Added {len(indicator_cols)} indicator columns\")\n",
    "        print(f\"    Rows with at least one indicator: {rows_with_indicators:,}\")\n",
    "        \n",
    "        # Merge with REER\n",
    "        df_merged = df_merged.merge(\n",
    "            df_reer_final,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        rows_with_reer = df_merged['REER'].notna().sum()\n",
    "        print(f\"  ✓ Added REER column\")\n",
    "        print(f\"    Rows with REER data: {rows_with_reer:,}\")\n",
    "        \n",
    "        # Save\n",
    "        output_filename = filename.replace('_final.csv', '_Additional.csv')\n",
    "        output_path = os.path.join(china_dir, output_filename)\n",
    "        df_merged.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"  ✓ Saved: {output_filename}\")\n",
    "        print(f\"    Total columns: {len(df_merged.columns)}\")\n",
    "        print(f\"    Indicator columns: {indicator_cols}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All files processed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNew columns added:\")\n",
    "print(\"  - gdp_constant_sa (if available in data)\")\n",
    "print(\"  - final_consumption_constant_sa\")\n",
    "print(\"  - gross_capital_formation_constant_sa\")\n",
    "print(\"  - changes_inventories_constant_sa\")\n",
    "print(\"  - REER (Real Effective Exchange Rate)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59698f06",
   "metadata": {},
   "source": [
    "### ADDITIONAL DATA 2023 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfed376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define directories and file paths\n",
    "input_dir = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\processed_input_data'\n",
    "indicators_file = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\Data\\china and gdp data\\china and gdp data\\df_long.csv'\n",
    "reer_file = r'C:\\Users\\wb636273\\OneDrive - WBG\\Documents\\AI4TRADE\\outputs\\EER_COUNTRIES.csv'\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Try different encodings\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df\n",
    "        except:\n",
    "            continue\n",
    "    raise Exception(f\"Could not read file: {file_path}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARATION: Loading reference data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load indicators data\n",
    "df_indicators = read_csv_with_encoding(indicators_file)\n",
    "print(f\"\\n✓ Loaded indicators data: {len(df_indicators):,} rows\")\n",
    "\n",
    "# Filter indicators\n",
    "indicator_keywords = ['GDP', 'consumption expenditure', 'capital formation', 'inventories']\n",
    "\n",
    "df_indicators_filtered = df_indicators[\n",
    "    (df_indicators['PRICE_TYPE'] == 'Constant prices') &\n",
    "    (df_indicators['S_ADJUSTMENT'] == 'Seasonally adjusted (SA)') &\n",
    "    (df_indicators['INDICATOR'].apply(lambda x: any(keyword.lower() in x.lower() for keyword in indicator_keywords)))\n",
    "].copy()\n",
    "\n",
    "print(f\"✓ Filtered indicators: {len(df_indicators_filtered):,} rows\")\n",
    "print(f\"Indicators found: {df_indicators_filtered['INDICATOR'].unique().tolist()}\")\n",
    "\n",
    "# Pivot indicators\n",
    "df_indicators_wide = df_indicators_filtered.pivot_table(\n",
    "    index=['ISO3', 'PERIOD'],\n",
    "    columns='INDICATOR',\n",
    "    values='VALUE',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns\n",
    "column_rename = {\n",
    "    'ISO3': 'country_id',\n",
    "    'PERIOD': 'month_id'\n",
    "}\n",
    "\n",
    "for col in df_indicators_wide.columns:\n",
    "    if 'GDP' in col or 'Gross domestic product' in col:\n",
    "        column_rename[col] = 'gdp_constant_sa'\n",
    "    elif 'Final consumption expenditure' in col:\n",
    "        column_rename[col] = 'final_consumption_constant_sa'\n",
    "    elif 'Gross capital formation' in col:\n",
    "        column_rename[col] = 'gross_capital_formation_constant_sa'\n",
    "    elif 'Changes in inventories' in col:\n",
    "        column_rename[col] = 'changes_inventories_constant_sa'\n",
    "\n",
    "df_indicators_wide = df_indicators_wide.rename(columns=column_rename)\n",
    "print(f\"✓ Pivoted indicators: {df_indicators_wide.shape}\")\n",
    "\n",
    "# Load REER data\n",
    "df_reer = read_csv_with_encoding(reer_file)\n",
    "print(f\"\\n✓ Loaded REER data: {len(df_reer)} rows\")\n",
    "\n",
    "# Filter for REER\n",
    "df_reer_filtered = df_reer[df_reer['INDICATOR'].str.contains('REER', case=False, na=False)].copy()\n",
    "\n",
    "# Get date columns\n",
    "date_columns = [col for col in df_reer_filtered.columns if '-M' in col]\n",
    "\n",
    "# Melt to long format\n",
    "df_reer_long = df_reer_filtered.melt(\n",
    "    id_vars=['COUNTRY.ID'],\n",
    "    value_vars=date_columns,\n",
    "    var_name='period_str',\n",
    "    value_name='REER'\n",
    ")\n",
    "\n",
    "# Convert period format\n",
    "df_reer_long['month_id'] = df_reer_long['period_str'].str.replace('-M', '').astype(int)\n",
    "df_reer_long = df_reer_long.rename(columns={'COUNTRY.ID': 'country_id'})\n",
    "df_reer_final = df_reer_long[['country_id', 'month_id', 'REER']].dropna(subset=['REER'])\n",
    "\n",
    "print(f\"✓ Prepared REER data: {df_reer_final.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1: Splitting files by trade flow...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Files to process\n",
    "files_to_process = [\n",
    "    'USA_2023_finale.csv',\n",
    "    'USA_2024_finale.csv',\n",
    "    'china_2023_finale.csv',\n",
    "    'china_2024_finale.csv'\n",
    "]\n",
    "\n",
    "split_files = []  # Keep track of split files for next step\n",
    "\n",
    "for filename in files_to_process:\n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"\\n⚠️  File not found: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file\n",
    "        df = read_csv_with_encoding(file_path)\n",
    "        print(f\"  Original rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Check if trade_flow_name exists\n",
    "        if 'trade_flow_name' not in df.columns:\n",
    "            print(f\"  ⚠️  'trade_flow_name' column not found!\")\n",
    "            continue\n",
    "        \n",
    "        # Get unique trade flows\n",
    "        trade_flows = df['trade_flow_name'].unique()\n",
    "        print(f\"  Trade flows found: {trade_flows}\")\n",
    "        \n",
    "        # Extract country and year from filename\n",
    "        # e.g., \"USA_2023_finale.csv\" -> \"USA\", \"2023\"\n",
    "        parts = filename.replace('_finale.csv', '').split('_')\n",
    "        country = parts[0]\n",
    "        year = parts[1]\n",
    "        \n",
    "        # Split by trade flow\n",
    "        for flow in trade_flows:\n",
    "            df_flow = df[df['trade_flow_name'] == flow].copy()\n",
    "            \n",
    "            # Create filename\n",
    "            flow_name = flow.lower()\n",
    "            output_filename = f\"{country}_{year}_{flow_name}_final.csv\"\n",
    "            output_path = os.path.join(input_dir, output_filename)\n",
    "            \n",
    "            # Save\n",
    "            df_flow.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            print(f\"    ✓ {output_filename}: {len(df_flow):,} rows\")\n",
    "            \n",
    "            # Track for next step\n",
    "            split_files.append(output_filename)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Adding indicators and REER to create _Additional files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename in split_files:\n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "    print(f\"\\n{filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file\n",
    "        df = read_csv_with_encoding(file_path)\n",
    "        df[\"country_id\"] = df[\"country_id\"].str.upper()\n",
    "        print(f\"  Original rows: {len(df):,}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        if 'country_id' not in df.columns or 'month_id' not in df.columns:\n",
    "            print(f\"  ⚠️  Required columns (country_id, month_id) not found!\")\n",
    "            print(f\"      Available columns: {df.columns.tolist()}\")\n",
    "            continue\n",
    "        \n",
    "        # Merge with indicators\n",
    "        df_merged = df.merge(\n",
    "            df_indicators_wide,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        indicator_cols = [col for col in df_merged.columns if '_constant_sa' in col]\n",
    "        rows_with_indicators = df_merged[indicator_cols].notna().any(axis=1).sum()\n",
    "        print(f\"  ✓ Added {len(indicator_cols)} indicator columns\")\n",
    "        print(f\"    Indicators: {indicator_cols}\")\n",
    "        print(f\"    Rows with at least one indicator: {rows_with_indicators:,}\")\n",
    "        \n",
    "        # Merge with REER\n",
    "        df_merged = df_merged.merge(\n",
    "            df_reer_final,\n",
    "            on=['country_id', 'month_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        rows_with_reer = df_merged['REER'].notna().sum()\n",
    "        print(f\"  ✓ Added REER column\")\n",
    "        print(f\"    Rows with REER data: {rows_with_reer:,}\")\n",
    "        \n",
    "        # Save\n",
    "        output_filename = filename.replace('_final.csv', '_Additional.csv')\n",
    "        output_path = os.path.join(input_dir, output_filename)\n",
    "        df_merged.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"  ✓ Saved: {output_filename}\")\n",
    "        print(f\"    Total columns: {len(df_merged.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All files processed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOutput files created:\")\n",
    "print(\"  Step 1 - Split by trade flow (8 files):\")\n",
    "for f in split_files:\n",
    "    print(f\"    - {f}\")\n",
    "print(\"\\n  Step 2 - Added indicators and REER (8 files):\")\n",
    "for f in split_files:\n",
    "    print(f\"    - {f.replace('_final.csv', '_Additional.csv')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capexdx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
